{
    "payload": {
        "prompt": "\r\n# Summary\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cert-manager\\uninstall.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\nkubectl delete namespace cert-manager\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ingress-nginx\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: ingress-nginx\r\n\r\nresources:\r\n- base/nginx-configuration.yaml\r\n# - https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.5.1/deploy/static/provider/cloud/deploy.yaml\r\n\r\nhelmCharts:\r\n- name: ingress-nginx\r\n  repo: https://kubernetes.github.io/ingress-nginx\r\n  version: 4.4.0\r\n  namespace: ingress-nginx\r\n  releaseName: ingress-nginx\r\n  includeCRDs: true\r\n  valuesMerge: merge\r\n  valuesInline:\r\n    ingress:\r\n      extra_args:\r\n        enable-ssl-passthrough: true\r\n    controller:\r\n      service:\r\n        annotations:\r\n          service.beta.kubernetes.io/azure-load-balancer-health-probe-request-path: /healthz\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-cluster-secretstores.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: cluster-secretstores\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/cluster-secretstores\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: external-secrets\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\templates\\deployment.yaml\r\n\r\n````yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: front\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels: \r\n      app: nginx\r\n  template:\r\n    metadata:\r\n      labels: \r\n        app: nginx\r\n    spec:\r\n      nodeSelector:\r\n        kubernetes.io/os: linux\r\n      containers:\r\n        - name: nginx\r\n          image: nginx:latest\r\n          resources:\r\n            requests:\r\n              cpu: 100m\r\n              memory: 128Mi\r\n            limits:\r\n              cpu: 250m\r\n              memory: 256Mi\r\n          ports:\r\n            - containerPort: 80\r\n          volumeMounts:\r\n            # - mountPath: \"/var/www/html\"\r\n            - mountPath: \"/usr/share/nginx/html\"\r\n              name: persistent-storage\r\n      volumes:\r\n        # https://github.com/kubernetes-sigs/blob-csi-driver/blob/master/deploy/example/e2e_usage.md\r\n        - name: persistent-storage\r\n          csi:\r\n            driver: blob.csi.azure.com\r\n            volumeAttributes:\r\n              containerName: \"$web\"\r\n              secretName: storage-account-connection\r\n              mountOptions: \"-o allow_other --file-cache-timeout-in-seconds=120\"\r\n\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cert-manager\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl create ns cert-manager\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\nWrite-Host \"Install complete, consider running verify.ps1\" -ForegroundColor \"Cyan\"\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-secrets\\uninstall.ps1\r\n\r\n````powershell\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\nkubectl delete ns external-secrets\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-cert-manager.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: cert-manager\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/cert-manager\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: cert-manager\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  ignoreDifferences:\r\n  - group: admissionregistration.k8s.io\r\n    kind: ValidatingWebhookConfiguration\r\n    name: cert-manager-webhook\r\n    jqPathExpressions:\r\n      - .webhooks[].namespaceSelector.matchExpressions[] | select(.key == \"control-plane\")\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\argocd\\base\\ingress.yaml\r\n\r\n````yaml\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: argo-cd-ui\r\n  annotations:\r\n    ingress.kubernetes.io/proxy-body-size: 100M\r\n    kubernetes.io/ingress.class: \"nginx\"\r\n    ingress.kubernetes.io/app-root: \"/\"\r\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\r\n    nginx.ingress.kubernetes.io/ssl-passthrough: \"true\"\r\n    nginx.ingress.kubernetes.io/backend-protocol: \"HTTPS\" \r\nspec:\r\n  tls:\r\n  - hosts:\r\n    - argocd.teamdman.ca\r\n    secretName: argocd-secret\r\n  rules:\r\n  - host: argocd.teamdman.ca\r\n    http:\r\n      paths:\r\n      - path: /\r\n        pathType: ImplementationSpecific\r\n        backend:\r\n          service:\r\n            name: argocd-server\r\n            port:\r\n              name: http\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\argocd\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\nkubectl create ns argocd\r\nkubectl apply -k .\r\n\r\nWrite-Host \"Waiting for initial secret to become available\" -ForegroundColor \"Cyan\"\r\nwhile ($true) {\r\n    $x = kubectl get secret -n argocd argocd-initial-admin-secret -o name --ignore-not-found\r\n    if ($x.Count -gt 0) {\r\n        break\r\n    }\r\n    Start-Sleep -Seconds 1\r\n}\r\n\r\n$pw = kubectl get secret -n argocd argocd-initial-admin-secret -o json | ConvertFrom-Json\r\n$pw = [System.Text.Encoding]::UTF8.GetString([System.Convert]::FromBase64String($pw.data.password))\r\nWrite-Host \"ArgoCD admin password is `\"${pw}`\"\"\r\n\r\nWrite-Host \"Waiting for certificate request to be approved\" -ForegroundColor \"Cyan\"\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-issuers\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\boilerplate.tf\r\n\r\n````terraform\r\nterraform {\r\n  backend \"azurerm\" {\r\n    resource_group_name  = \"Terraform\"\r\n    storage_account_name = \"terraform9201\"\r\n    container_name       = \"tfstate\"\r\n    key                  = \"ca.teamdman.tfstate\"\r\n    # subscription_id = \"\"\r\n  }\r\n  required_providers {\r\n    azurerm = {\r\n      source  = \"hashicorp/azurerm\"\r\n      version = \">=3.77.0\"\r\n    }\r\n    azuread = {\r\n      source  = \"hashicorp/azuread\"\r\n      version = \">=2.44.1\"\r\n    }\r\n    github = {\r\n      source  = \"integrations/github\"\r\n      version = \">=5.40.0\"\r\n    }\r\n    random = {\r\n      source  = \"hashicorp/random\"\r\n      version = \">=3.5.1\"\r\n    }\r\n  }\r\n}\r\n\r\nlocals {\r\n  dotenv = { for tuple in regexall(\"(.*)=(.*?)\\\\s*\", file(\".env\")) : tuple[0] => tuple[1] }\r\n}\r\n\r\n# Configure the Microsoft Azure Provider\r\nprovider \"azurerm\" {\r\n  features {}\r\n}\r\n\r\nprovider \"github\" {\r\n  owner = \"teamdman-ca\"\r\n  token = local.dotenv.github_token\r\n}\r\n\r\ndata \"azurerm_client_config\" \"current\" {\r\n\r\n}\r\n\r\ndata \"azuread_client_config\" \"current\" {\r\n\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\template\\apply.ps1\r\n\r\n````powershell\r\n$subs = az account list | ConvertFrom-Json\r\n\r\n$active = $null;\r\n$desired = $null;\r\n$desired_name = \"AAFC VSE Benefit\"\r\n\r\nforeach($sub in $subs) {\r\n    if ($sub.isDefault) {\r\n        $active = $sub\r\n    }\r\n    if ($sub.name -eq $desired_name) {\r\n        $desired = $sub\r\n    }\r\n}\r\n\r\nWrite-Host \"Currently have $($active.name) selected\"\r\n\r\nif ($active.id -ne $desired.id) {\r\n    Write-Host \"Switching subscription to $($desired.name)\"\r\n    az account set --subscription $desired.id\r\n} else {\r\n    Write-Host \"Correct subscription already selected\"\r\n}\r\n\r\nWrite-Host \"Currently on subscription $(az account show --query \"name\" -o tsv)\"\r\n\r\nWrite-Host \"Beginning terraform apply\" -ForegroundColor Cyan\r\nterraform apply\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\service principal.tf\r\n\r\n````terraform\r\nresource \"azuread_application\" \"site_deployer\" {\r\n  display_name = \"github_cicd_pipeline\"\r\n  owners       = [data.azuread_client_config.current.object_id]\r\n}\r\n\r\nresource \"azuread_application_password\" \"site_deployer_password\" {\r\n  application_object_id = azuread_application.site_deployer.object_id\r\n}\r\n\r\nresource \"azuread_service_principal\" \"site_deployer\" {\r\n  application_id = azuread_application.site_deployer.application_id\r\n  owners         = azuread_application.site_deployer.owners\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\gifts\\boilerplate.tf\r\n\r\n````terraform\r\nterraform {\r\n  backend \"azurerm\" {\r\n    resource_group_name  = \"Terraform\"\r\n    storage_account_name = \"terraform9201\"\r\n    container_name       = \"tfstate\"\r\n    key                  = \"ca.teamdman.tfstate\"\r\n    subscription_id      = \"{{subscription_id}}\"\r\n  }\r\n  required_providers {\r\n    azurerm = {\r\n      source  = \"hashicorp/azurerm\"\r\n      version = \">=3.77.0\"\r\n    }\r\n    azuread = {\r\n      source  = \"hashicorp/azuread\"\r\n      version = \">=2.44.1\"\r\n    }\r\n    github = {\r\n      source  = \"integrations/github\"\r\n      version = \">=5.40.0\"\r\n    }\r\n    random = {\r\n      source  = \"hashicorp/random\"\r\n      version = \">=3.5.1\"\r\n    }\r\n  }\r\n}\r\n\r\nlocals {\r\n  dotenv = { for tuple in regexall(\"(.*)=(.*?)\\\\s*\", file(\".env\")) : tuple[0] => tuple[1] }\r\n}\r\n\r\n# Configure the Microsoft Azure Provider\r\nprovider \"azurerm\" {\r\n  features {}\r\n}\r\n\r\nprovider \"github\" {\r\n  owner = \"teamdman-ca\"\r\n  token = local.dotenv.github_token\r\n}\r\n\r\ndata \"azurerm_client_config\" \"current\" {\r\n\r\n}\r\n\r\ndata \"azuread_client_config\" \"current\" {\r\n\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\deployment-backend.yaml\r\n\r\n````yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: gifts-backend\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels: \r\n      app: gifts-backend\r\n  template:\r\n    metadata:\r\n      labels: \r\n        app: gifts-backend\r\n        azure.workload.identity/use: \"true\"\r\n    spec:\r\n      nodeSelector:\r\n        kubernetes.io/os: linux\r\n      serviceAccountName: gifts-sa\r\n      containers:\r\n        - name: gifts-backend\r\n          image: teamdman.azurecr.io/gifts-backend:latest\r\n          resources:\r\n            requests:\r\n              cpu: 100m\r\n              memory: 128Mi\r\n            limits:\r\n              cpu: 250m\r\n              memory: 256Mi\r\n          ports:\r\n            - containerPort: 80\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-issuers\\base\\clusterissuer-letsencrypt-prod.yaml\r\n\r\n````yaml\r\napiVersion: cert-manager.io/v1\r\nkind: ClusterIssuer\r\nmetadata:\r\n  name: letsencrypt-prod\r\nspec:\r\n  acme:\r\n    email: TeamDman9201@gmail.com\r\n    server: https://acme-v02.api.letsencrypt.org/directory\r\n    privateKeySecretRef:\r\n      # Secret resource that will be used to store the account's private key.\r\n      name: cert-manager-issuer-account-key-prod\r\n    solvers:\r\n    - selector: {}\r\n      dns01:\r\n        azureDNS:\r\n          subscriptionID: 134ae9a1-7bc2-41f7-a2c7-a1e55f54ed04\r\n          resourceGroupName: ca.teamdman\r\n          hostedZoneName: teamdman.ca\r\n          environment: AzurePublicCloud\r\n          managedIdentity:\r\n            clientID: 860af6b9-4135-4814-bf81-16f3e8ebcbdd\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-secretstores\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-secretstores\\uninstall.ps1\r\n\r\n````powershell\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-external-dns-azurewebsiets.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: external-dns-azurewebsiets\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/external-dns-azurewebsiets\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: external-dns-azurewebsiets\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: argocd\r\n\r\nresources:\r\n- base/core/project-core.yaml\r\n- base/core/app-argocd.yaml\r\n- base/core/app-app-of-apps.yaml\r\n- base/core/app-blob-csi-driver.yaml\r\n- base/core/app-cert-manager.yaml\r\n- base/core/app-cluster-issuers.yaml\r\n- base/core/app-cluster-secretstores.yaml\r\n- base/core/app-ingress-nginx.yaml\r\n- base/core/app-external-secrets.yaml\r\n- base/core/app-external-dns-teamdman.yaml\r\n- base/core/app-external-dns-azurewebsiets.yaml\r\n- base/core/app-workload-identity-webhook.yaml\r\n- base/teamdman/project-teamdman.yaml\r\n- base/teamdman/app-teamdman-www.yaml\r\n- base/teamdman/app-teamdman-gifts.yaml\r\n- base/azurewebsiets/project-azurewebsiets.yaml\r\n- base/azurewebsiets/app-azurewebsiets-www.yaml\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\argocd\\overlays\\argocd-cm.yaml\r\n\r\n````yaml\r\napiVersion: v1\r\nkind: ConfigMap\r\nmetadata:\r\n  name: argocd-cm\r\ndata:\r\n  server.insecure: \"true\"\r\n  # admin.enabled: \"false\"\r\n  statusbadge.enabled: \"true\"\r\n  users.anonymous.enabled: \"false\"\r\n  kustomize.buildOptions: --enable-helm\r\n  url: \"https://argocd.teamdman.ca\"\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\argocd\\uninstall.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\nkubectl delete -k .\r\nkubectl delete ns argocd\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\email-forwarder\\install.ps1\r\n\r\n````powershell\r\nkubectl apply -f deploy.yaml\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-dns-azurewebsiets\\uninstall.ps1\r\n\r\n````powershell\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\nkubectl delete ns external-dns-azurewebsiets\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\src\\modules\\cluster\\aks.tf\r\n\r\n````terraform\r\nvariable \"cluster_name\" {\r\n  type = string\r\n}\r\nvariable \"cluster_dns_prefix\" {\r\n  type = string\r\n}\r\nresource \"azurerm_kubernetes_cluster\" \"main\" {\r\n  resource_group_name       = azurerm_resource_group.main.name\r\n  location                  = azurerm_resource_group.main.location\r\n  tags                      = azurerm_resource_group.main.tags\r\n  name                      = var.cluster_name\r\n  dns_prefix                = var.cluster_dns_prefix\r\n  sku_tier                  = \"Free\"\r\n  kubernetes_version        = \"1.28\"\r\n  oidc_issuer_enabled       = true\r\n  workload_identity_enabled = true\r\n  role_based_access_control_enabled = true\r\n\r\n  default_node_pool {\r\n    name       = \"default\"\r\n    node_count = 1\r\n    vm_size    = \"standard_b2s\"\r\n    upgrade_settings {\r\n      max_surge = \"10%\"\r\n    }\r\n  }\r\n\r\n  identity {\r\n    type = \"SystemAssigned\"\r\n  }\r\n\r\n  \r\n  network_profile {\r\n    network_plugin = \"kubenet\"\r\n  }\r\n\r\n\r\n  azure_active_directory_role_based_access_control {\r\n    # There is a deprecation warning here, we must continue to specify managed=true until it is removed\r\n    managed            = true\r\n    azure_rbac_enabled = true\r\n    admin_group_object_ids = [\r\n      azuread_group.admins.object_id\r\n    ]\r\n  }\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\keyvault.tf\r\n\r\n````terraform\r\nresource \"azurerm_key_vault\" \"main\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  location            = \"canadaeast\"\r\n  tenant_id           = data.azurerm_client_config.current.tenant_id\r\n  sku_name            = \"standard\"\r\n  name                = \"ca-teamdman\"\r\n}\r\n\r\nresource \"azurerm_key_vault_secret\" \"main\" {\r\n  for_each = {\r\n    \"storage-account-name\" = azurerm_storage_account.main.name\r\n    \"storage-account-key\"  = azurerm_storage_account.main.primary_access_key\r\n  }\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  name         = each.key\r\n  value        = each.value\r\n  depends_on = [\r\n    azurerm_key_vault_access_policy.me\r\n  ]\r\n}\r\n\r\nresource \"azurerm_key_vault_access_policy\" \"external-secrets-operator\" {\r\n  key_vault_id       = azurerm_key_vault.main.id\r\n  secret_permissions = [\"Get\", \"List\"]\r\n  tenant_id          = azurerm_key_vault.main.tenant_id\r\n  object_id          = azurerm_kubernetes_cluster.main.kubelet_identity[0].object_id\r\n}\r\n\r\n###\r\n\r\n# this does not scale to team projects\r\nlocals {\r\n  key_vault_reader_object_ids = [\r\ndata.azurerm_client_config.current.object_id\r\n  ]\r\n}\r\nresource \"azurerm_key_vault_access_policy\" \"me\" {\r\n  for_each = local.key_vault_reader_object_ids\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  secret_permissions = [\r\n    \"Backup\", \"Delete\", \"Get\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Set\"\r\n  ]\r\n  certificate_permissions = [\r\n    \"Backup\", \"Create\", \"Delete\", \"DeleteIssuers\", \"Get\", \"GetIssuers\", \"Import\", \"List\", \"ListIssuers\", \"ManageContacts\", \"ManageIssuers\", \"Purge\", \"Recover\", \"Restore\", \"SetIssuers\", \"Update\"\r\n  ]\r\n  key_permissions = [\r\n    \"Backup\", \"Create\", \"Decrypt\", \"Delete\", \"Encrypt\", \"Get\", \"Import\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Sign\", \"UnwrapKey\", \"Update\", \"Verify\", \"WrapKey\"\r\n  ]\r\n  tenant_id = azurerm_key_vault.main.tenant_id\r\n  object_id = each.value\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\README.md\r\n\r\n````markdown\r\n[Attach an ACR to an AKS cluster](https://learn.microsoft.com/en-us/azure/aks/cluster-container-registry-integration?tabs=azure-cli) by running `az aks update -n teamdman-aks -g ca.teamdman --attach-acr teamdman`\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\outputs.tf\r\n\r\n````terraform\r\n# # output \"service_principal_password\" {\r\n# #   sensitive = true\r\n# #   value     = azuread_application_password.site_deployer_password.value\r\n# # }\r\n\r\n# # terraform intellisense only works at topmost level unfortunately\r\n# # https://github.com/hashicorp/vscode-terraform/issues/1246\r\n\r\n# output \"external_secrets_operator_config\" {\r\n#   value = {\r\n#     tenant_id           = data.azurerm_client_config.current.tenant_id\r\n#     subscription_id     = data.azurerm_client_config.current.subscription_id\r\n#     key_vault_uri       = azurerm_key_vault.main.vault_uri\r\n#     managed_identity_id = azurerm_kubernetes_cluster.main.kubelet_identity[0].client_id\r\n#     # managed_identity_id = data.azurerm_kubernetes_cluster.main.kubelet_identity[0].object_id\r\n#   }\r\n# }\r\n\r\n# output \"letsencrypt_config\" {\r\n#   value = {\r\n#     subscriptionID    = data.azurerm_client_config.current.subscription_id\r\n#     resourceGroupName = azurerm_resource_group.main.name\r\n#     hostedZoneName    = azurerm_dns_zone.main.name\r\n#     managedIdentity = {\r\n#       clientID = azurerm_kubernetes_cluster.main.kubelet_identity[0].client_id\r\n#     }\r\n#   }\r\n# }\r\n\r\n# output \"external_dns_config\" {\r\n#   value = {\r\n#     resourceGroup               = azurerm_resource_group.main.name\r\n#     tenandId                    = data.azurerm_client_config.current.tenant_id\r\n#     subscriptionId              = data.azurerm_client_config.current.subscription_id\r\n#     useManagedIdentityExtension = true\r\n#     userAssignedIdentityID      = azurerm_kubernetes_cluster.main.kubelet_identity[0].client_id\r\n#   }\r\n# }\r\n\r\n# output \"gifts_workload_identity\" {\r\n#   value = {\r\n#     clientId = azurerm_user_assigned_identity.gifts.client_id\r\n#   }\r\n# }\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-external-dns-teamdman.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: external-dns-teamdman\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/external-dns-teamdman\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: external-dns-teamdman\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\teamdman\\project-teamdman.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: AppProject\r\nmetadata:\r\n  name: teamdman\r\nspec:\r\n  description: Core resources to the cluster\r\n\r\n  # Allow manifests to deploy from any Git repos\r\n  sourceRepos:\r\n  - '*'\r\n\r\n  destinations:\r\n  - namespace: teamdman\r\n    server: https://kubernetes.default.svc\r\n  - namespace: teamdman-gifts\r\n    server: https://kubernetes.default.svc\r\n\r\n  # Deny all cluster-scoped resources from being created, except for Namespace\r\n  clusterResourceWhitelist:\r\n  - group: ''\r\n    kind: Namespace\r\n\r\n  # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy\r\n  namespaceResourceBlacklist:\r\n  - group: ''\r\n    kind: ResourceQuota\r\n  - group: ''\r\n    kind: LimitRange\r\n  - group: ''\r\n    kind: NetworkPolicy\r\n\r\n  # # Deny all namespaced-scoped resources from being created, except for Deployment and StatefulSet\r\n  # namespaceResourceWhitelist:\r\n  # - group: 'apps'\r\n  #   kind: Deployment\r\n  # - group: 'apps'\r\n  #   kind: StatefulSet\r\n\r\n  # Enables namespace orphaned resource monitoring.\r\n  orphanedResources:\r\n    warn: false\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-app-of-apps.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: app-of-apps\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/app-of-apps\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: argocd\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\Chart.yaml\r\n\r\n````yaml\r\napiVersion: v2\r\nname: ca-teamdman\r\ndescription: Static site for teamdman.ca\r\n\r\ntype: application\r\n\r\nversion: \"0.1.0\"\r\nappVersion: \"1.0.0\"\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-secretstores\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: external-secrets\r\n\r\nresources:\r\n- base/cluster-secret-store.yaml\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-dns-azurewebsiets\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl create ns external-dns-azurewebsiets\r\n# this will automatically create the charts/ directory\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\rg.tf\r\n\r\n````terraform\r\nresource \"azurerm_resource_group\" \"main\" {\r\n  name     = \"MyKubernetes\"\r\n  location = \"canadaeast\"\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\src\\boilerplate.tf\r\n\r\n````terraform\r\nterraform {\r\n  backend \"azurerm\" {\r\n    subscription_id      = \"6cb7032f-2437-4f5e-91e8-676cb67e5444\"\r\n    resource_group_name  = \"CACN-Terraform-PROD-RG\"\r\n    storage_account_name = \"terraformproddwvc87\"\r\n    container_name       = \"statefiles\"\r\n    key                  = \"teamdman.tfstate\"\r\n  }\r\n}\r\n\r\nprovider \"azurerm\" {\r\n  features {}\r\n  subscription_id = \"6cb7032f-2437-4f5e-91e8-676cb67e5444\"\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-issuers\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: default\r\n\r\nresources:\r\n  - base/clusterissuer-letsencrypt-prod.yaml\r\n  - base/clusterissuer-letsencrypt-staging.yaml\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\init\\main.tf\r\n\r\n````terraform\r\nprovider \"azurerm\" {\r\n  features {}\r\n  subscription_id = \"6cb7032f-2437-4f5e-91e8-676cb67e5444\"\r\n}\r\n\r\nresource \"azurerm_resource_group\" \"main\" {\r\n  name     = \"CACN-Terraform-PROD-RG\"\r\n  location = \"canadacentral\"\r\n  tags = {\r\n    environment = \"Production\"\r\n  }\r\n}\r\n\r\nresource \"random_string\" \"suffix\" {\r\n  special = false\r\n  upper   = false\r\n  length  = 6\r\n}\r\n\r\ndata \"http\" \"myip\" {\r\n  url = \"https://ipv4.icanhazip.com\"\r\n}\r\n\r\nresource \"azurerm_storage_account\" \"main\" {\r\n  name                          = \"terraformprod${random_string.suffix.result}\"\r\n  resource_group_name           = azurerm_resource_group.main.name\r\n  location                      = azurerm_resource_group.main.location\r\n  account_tier                  = \"Standard\"\r\n  account_replication_type      = \"LRS\"\r\n  tags                          = azurerm_resource_group.main.tags\r\n  network_rules {\r\n    default_action = \"Deny\"\r\n    # bypass = [\r\n    #     \"${chomp(data.http.myip.response_body)}/32\"\r\n    # ]\r\n    ip_rules = [\r\n      \"${chomp(sensitive(data.http.myip.response_body))}\"\r\n    ]\r\n  }\r\n}\r\n\r\nresource \"azurerm_storage_container\" \"main\" {\r\n  name                 = \"statefiles\"\r\n  storage_account_name = azurerm_storage_account.main.name\r\n}\r\n\r\noutput \"storage_account_name\" {\r\n  value = azurerm_storage_account.main.name\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\teamdman\\app-teamdman-gifts.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: teamdman-gifts\r\n  namespace: argocd\r\nspec:\r\n  project: teamdman\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/ca-teamdman-gifts\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: teamdman-gifts\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-secrets\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: external-secrets\r\n\r\nhelmCharts:\r\n- name: external-secrets\r\n  repo: https://charts.external-secrets.io\r\n  version: 0.6.1\r\n  releaseName: external-secrets\r\n  includeCRDs: true\r\n  namespace: external-secrets\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\storage accounts.tf\r\n\r\n````terraform\r\nresource \"azurerm_storage_account\" \"main\" {\r\n  resource_group_name      = azurerm_resource_group.main.name\r\n  location                 = \"canadaeast\"\r\n  name                     = \"teamdman\"\r\n  account_replication_type = \"LRS\"\r\n  account_tier             = \"Standard\"\r\n}\r\n\r\nresource \"azurerm_storage_container\" \"web\" {\r\n  storage_account_name = azurerm_storage_account.main.name\r\n  name                 = \"$web\"\r\n}\r\n\r\nresource \"azurerm_role_assignment\" \"site_write\" {\r\n  principal_id         = azuread_service_principal.site_deployer.object_id\r\n  scope                = azurerm_storage_account.main.id\r\n  role_definition_name = \"Storage Blob Data Contributor\"\r\n}\r\n\r\n\r\nresource \"azurerm_storage_table\" \"gifts\" {\r\n  name                 = \"gifts\"\r\n  storage_account_name = azurerm_storage_account.main.name\r\n}\r\nresource \"azurerm_storage_table\" \"gifts_dev\" {\r\n  name                 = \"giftsdev\"\r\n  storage_account_name = azurerm_storage_account.main.name\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\nkubectl apply -k .\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ingress-nginx\\base\\nginx-configuration.yaml\r\n\r\n````yaml\r\n# apiVersion: v1\r\n# kind: ConfigMap\r\n# metadata:\r\n#   labels:\r\n#     app: ingress-nginx\r\n#   name: nginx-configuration\r\n#   namespace: ingress-nginx\r\n# data:\r\n#   hsts: \"false\"\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\cdn.tf\r\n\r\n````terraform\r\n# resource \"azurerm_cdn_profile\" \"main\" {\r\n#   resource_group_name = azurerm_resource_group.main.name\r\n#   location            = \"canadaeast\"\r\n#   sku                 = \"Standard_Microsoft\"\r\n#   name                = \"ca-teamdman\"\r\n# }\r\n\r\n# locals {\r\n#   host_name = regex(\"^https://(.*)/$\", azurerm_storage_account.main.primary_web_endpoint)[0]\r\n# }\r\n\r\n# resource \"azurerm_cdn_endpoint\" \"root\" {\r\n#   resource_group_name = azurerm_cdn_profile.main.resource_group_name\r\n#   profile_name        = azurerm_cdn_profile.main.name\r\n#   location            = \"canadaeast\"\r\n#   name                = \"ca-teamdman\"\r\n#   origin {\r\n#     name      = \"origin1\"\r\n#     host_name = local.host_name\r\n#   }\r\n#   origin_host_header = local.host_name\r\n\r\n#   delivery_rule {\r\n#     name  = \"httpsredirect\"\r\n#     order = 1\r\n\r\n#     request_scheme_condition {\r\n#       match_values = [\r\n#         \"HTTP\",\r\n#       ]\r\n#       negate_condition = false\r\n#       operator         = \"Equal\"\r\n#     }\r\n\r\n#     url_redirect_action {\r\n#       protocol      = \"Https\"\r\n#       redirect_type = \"PermanentRedirect\"\r\n#     }\r\n#   }\r\n\r\n#   lifecycle {\r\n#     ignore_changes = [origin, is_compression_enabled]\r\n#   }\r\n# }\r\n\r\n# resource \"azurerm_cdn_endpoint_custom_domain\" \"root\" {\r\n#   cdn_endpoint_id = azurerm_cdn_endpoint.root.id\r\n#   name            = \"teamdman-ca\"\r\n#   host_name       = \"teamdman.ca\"\r\n#   user_managed_https {\r\n#     key_vault_certificate_id = \"https://ca-teamdman.vault.azure.net/certificates/teamdman-ca\"\r\n#   }\r\n#   depends_on = [\r\n#     azurerm_dns_a_record.root\r\n#   ]\r\n# }\r\n\r\n# resource \"azurerm_role_assignment\" \"cdn_purge\" {\r\n#   principal_id         = azuread_service_principal.site_deployer.object_id\r\n#   scope                = azurerm_cdn_endpoint.root.id\r\n#   role_definition_name = \"CDN Endpoint Contributor\"\r\n# }\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\readme.md\r\n\r\n````markdown\r\n# Prerequisites\r\n\r\nEnsure blob-csi-driver is installed\r\n\r\nhttps://github.com/kubernetes-sigs/blob-csi-driver\r\n\r\n# Other\r\n\r\nhttps://gist.github.com/bigbrozer/07d10d1b9eb238d909a45ee9277e2f90\r\n\r\nhttps://github.com/kubernetes-sigs/blob-csi-driver/tree/master/charts\r\n\r\nkubectl create -f https://raw.githubusercontent.com/kubernetes-sigs/blob-csi-driver/master/deploy/example/storageclass-blobfuse.yaml\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\uninstall.ps1\r\n\r\n````powershell\r\nhelm delete \"teamdman\" --namespace \"teamdman\"\r\nkubectl delete namespace \"teamdman\"\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\template\\rg.tf\r\n\r\n````terraform\r\nresource \"azurerm_resource_group\" \"main\" {\r\n  name     = \"MyKubernetes\"\r\n  location = \"canadaeast\"\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\azurewebsiets\\app-azurewebsiets-www.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: azurewebsiets-www\r\n  namespace: argocd\r\nspec:\r\n  project: azurewebsiets\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/azurewebsiets-net/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/net-azurewebsiets-www\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: azurewebsiets\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-cluster-issuers.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: cluster-issuers\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/cluster-issuers\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: default\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\argocd\\base\\certificate.yaml\r\n\r\n````yaml\r\napiVersion: cert-manager.io/v1\r\nkind: Certificate\r\nmetadata:\r\n  name: argo-cd-cert\r\nspec:\r\n  secretName: argocd-secret\r\n  issuerRef:\r\n    kind: ClusterIssuer\r\n    name: letsencrypt-prod\r\n    # name: letsencrypt-staging\r\n    # name: selfsigned-issuer\r\n  commonName: argocd.teamdman.ca\r\n  dnsNames:\r\n  - argocd.teamdman.ca\r\n  - temp1.teamdman.ca\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: teamdman-gifts\r\n\r\nresources:\r\n- base/service-account.yaml\r\n- base/ingress.yaml\r\n- base/deployment-backend.yaml\r\n- base/deployment-frontend.yaml\r\n- base/service-backend.yaml\r\n- base/service-frontend.yaml\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-dns-teamdman\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl create ns external-dns-teamdman\r\n# this will automatically create the charts/ directory\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\src\\modules\\cluster\\dns.tf\r\n\r\n````terraform\r\nresource \"azurerm_dns_zone\" \"main\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  name                = \"azurewebsiets.net\"\r\n}\r\n\r\n# resource \"azurerm_role_assignment\" \"dns\" {\r\n#   # principal_id         = data.azurerm_kubernetes_cluster.main.identity[0].principal_id\r\n#   principal_id         = azurerm_kubernetes_cluster.main.kubelet_identity[0].object_id\r\n#   scope                = azurerm_dns_zone.main.id\r\n#   role_definition_name = \"DNS Zone Contributor\"\r\n# }\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\teamdmanwebsite\\github secrets.tf\r\n\r\n````terraform\r\ndata \"github_repository\" \"teamdman_content\" {\r\n  full_name = \"teamdman-ca/site-content\"\r\n}\r\n\r\nlocals {\r\n  secrets = {\r\n    service_principal_creds = jsonencode({\r\n      clientId                   = azuread_application.site_deployer.application_id\r\n      clientSecret               = azuread_application_password.site_deployer_password.value\r\n      tenantId                   = data.azuread_client_config.current.tenant_id\r\n      subscriptionId             = data.azurerm_client_config.current.subscription_id\r\n      resourceManagerEndpointUrl = \"https://management.azure.com/\"\r\n    })\r\n    storage_account_name   = azurerm_storage_account.main.name\r\n    storage_container_name = azurerm_storage_container.web.name\r\n  }\r\n}\r\n\r\nresource \"github_actions_organization_secret\" \"cicd\" {\r\n  for_each                = local.secrets\r\n  selected_repository_ids = [data.github_repository.teamdman_content.repo_id]\r\n  secret_name             = each.key\r\n  visibility              = \"selected\"\r\n  plaintext_value         = each.value\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\template\\main.tf\r\n\r\n````terraform\r\n{{warning_header}}\r\nterraform {\r\n  backend \"azurerm\" {\r\n    resource_group_name  = \"Terraform\"\r\n    storage_account_name = \"terraform9201\"\r\n    container_name       = \"tfstate\"\r\n    key                  = \"mykubernetes.tfstate\"\r\n    subscription_id      = \"{{subscription_id}}\"\r\n  }\r\n  required_providers {\r\n    azurerm = {\r\n      source  = \"hashicorp/azurerm\"\r\n      version = \">=3.34.0\"\r\n    }\r\n    azuread = {\r\n      source  = \"hashicorp/azuread\"\r\n      version = \">=2.31.0\"\r\n    }\r\n    github = {\r\n      source  = \"integrations/github\"\r\n      version = \">=4.26.1\"\r\n    }\r\n    random = {\r\n      source  = \"hashicorp/random\"\r\n      version = \">=3.4.3\"\r\n    }\r\n  }\r\n}\r\n\r\nprovider \"azurerm\" {\r\n  features {}\r\n  subscription_id = \"{{subscription_id}}\"\r\n}\r\n\r\ndata \"azurerm_client_config\" \"current\" {\r\n\r\n}\r\n\r\ndata \"azuread_client_config\" \"current\" {\r\n\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-workload-identity-webhook.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: workload-identity-webhook\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/workload-identity-webhook\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: azure-workload-identity-system\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\project-core.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: AppProject\r\nmetadata:\r\n  name: core\r\nspec:\r\n  description: Core resources to the cluster\r\n\r\n  # Allow manifests to deploy from any Git repos\r\n  sourceRepos:\r\n  - '*'\r\n\r\n  destinations:\r\n  - namespace: \"*\"\r\n    server: https://kubernetes.default.svc\r\n\r\n  # clusterResourceBlacklist: []\r\n  # namespaceResourceBlacklist: []\r\n  clusterResourceWhitelist:\r\n    - group: \"*\"\r\n      kind: \"*\"\r\n  namespaceResourceWhitelist:\r\n    - group: \"*\" \r\n      kind: \"*\"\r\n\r\n  # # Deny all cluster-scoped resources from being created, except for Namespace\r\n  # clusterResourceWhitelist:\r\n  # - group: ''\r\n  #   kind: Namespace\r\n\r\n  # # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy\r\n  # namespaceResourceBlacklist:\r\n  # - group: ''\r\n  #   kind: ResourceQuota\r\n  # - group: ''\r\n  #   kind: LimitRange\r\n  # - group: ''\r\n  #   kind: NetworkPolicy\r\n\r\n  # # Deny all namespaced-scoped resources from being created, except for Deployment and StatefulSet\r\n  # namespaceResourceWhitelist:\r\n  # - group: 'apps'\r\n  #   kind: Deployment\r\n  # - group: 'apps'\r\n  #   kind: StatefulSet\r\n\r\n  # Enables namespace orphaned resource monitoring.\r\n  orphanedResources:\r\n    warn: false\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\ingress.yaml\r\n\r\n````yaml\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: gifts\r\n  annotations:\r\n    cert-manager.io/cluster-issuer: letsencrypt-prod\r\n    # cert-manager.io/cluster-issuer: letsencrypt-staging\r\n    nginx.ingress.kubernetes.io/force-ssl-redirect: \"true\"\r\n    # cert-manager.io/issuer: letsencrypt-prod\r\nspec:\r\n  ingressClassName: nginx\r\n  # This section is only required if TLS is to be enabled for the Ingress\r\n  tls:\r\n    - hosts:\r\n        - gifts.raddest.ca\r\n      secretName: gifts-raddest-tls\r\n  rules:\r\n    - host: gifts.raddest.ca\r\n      http:\r\n        paths:\r\n          - path: /api/\r\n            pathType: Prefix\r\n            backend:\r\n              service:\r\n                name: gifts-backend\r\n                port:\r\n                  number: 80\r\n          - path: /\r\n            pathType: Prefix\r\n            backend:\r\n              service:\r\n                name: gifts-frontend\r\n                port:\r\n                  number: 80\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\values.yaml\r\n\r\n````yaml\r\nsub: subscription id\r\nrg: resource group name\r\ndns: dns name\r\nclientId: clientId\r\nstorageAccountName: azurewebsiets\r\nstorageAccountKey: abc\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cert-manager\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\n\r\n## don't specify namespace\r\n## https://github.com/cert-manager/cert-manager/issues/5471\r\n# namespace: cert-manager\r\n\r\n# resources:\r\n# - https://github.com/cert-manager/cert-manager/releases/download/v1.10.1/cert-manager.yaml\r\n# --cluster-issuer-ambient-credentials     \r\n# --issuer-ambient-credentials    \r\n\r\nhelmCharts:\r\n- name: cert-manager\r\n  repo: https://charts.jetstack.io\r\n  version: v1.8.2\r\n  releaseName: cert-manager\r\n  valuesInline:\r\n    installCRDs: true\r\n    extraArgs:\r\n    - --cluster-issuer-ambient-credentials\r\n    - --issuer-ambient-credentials\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-dns-teamdman\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: external-dns-teamdman\r\n\r\nhelmCharts:\r\n- name: external-dns\r\n  repo: https://charts.bitnami.com/bitnami\r\n  version: 6.12.1\r\n  releaseName: external-dns\r\n  namespace: external-dns-teamdman\r\n  valuesInline:\r\n    provider: azure\r\n    policy: sync\r\n    domainFilters:\r\n      - teamdman.ca # todo: merge external-dns namespaces into single one to manage all\r\n    azure:\r\n      resourceGroup: ca.teamdman\r\n      tenantId: 2e831e5f-9c6e-41a7-b295-50499684ba63\r\n      subscriptionId: 134ae9a1-7bc2-41f7-a2c7-a1e55f54ed04\r\n      useManagedIdentityExtension: true\r\n      userAssignedIdentityID: 860af6b9-4135-4814-bf81-16f3e8ebcbdd\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-ingress-nginx.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: ingress-nginx\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/ingress-nginx\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: ingress-nginx\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\service-frontend.yaml\r\n\r\n````yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: gifts-frontend\r\nspec:\r\n  ports:\r\n    - port: 80\r\n      targetPort: 3000\r\n  selector: \r\n    app: gifts-frontend\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-dns-teamdman\\uninstall.ps1\r\n\r\n````powershell\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\nkubectl delete ns external-dns-teamdman\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\README.md\r\n\r\n````markdown\r\n# TeamDman Infrastructure as Code\r\n\r\nAhoy!\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\acr.tf\r\n\r\n````terraform\r\nresource \"azurerm_container_registry\" \"main\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  name                = \"teamdman\"\r\n  sku                 = \"Standard\"\r\n  location            = \"canadacentral\"\r\n}\r\n\r\nresource \"azurerm_role_assignment\" \"acrpull\" {\r\n  scope                = azurerm_container_registry.main.id\r\n  role_definition_name = \"AcrPull\"\r\n  principal_id         = azurerm_kubernetes_cluster.main.kubelet_identity.0.object_id\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\aks.tf\r\n\r\n````terraform\r\nresource \"azurerm_kubernetes_cluster\" \"main\" {\r\n  resource_group_name       = azurerm_resource_group.main.name\r\n  location                  = \"canadacentral\"\r\n  name                      = \"teamdman-aks\"\r\n  sku_tier                  = \"Free\"\r\n  dns_prefix                = \"sharedcluster\"\r\n  kubernetes_version        = \"1.24.6\"\r\n  oidc_issuer_enabled       = true\r\n  workload_identity_enabled = true\r\n  default_node_pool {\r\n    name       = \"default\"\r\n    node_count = 1\r\n    vm_size    = \"standard_b2s\"\r\n  }\r\n\r\n\r\n  identity {\r\n    type = \"SystemAssigned\"\r\n  }\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-dns-azurewebsiets\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: external-dns-azurewebsiets\r\n\r\nhelmCharts:\r\n- name: external-dns\r\n  repo: https://charts.bitnami.com/bitnami\r\n  version: 6.12.1\r\n  releaseName: external-dns\r\n  namespace: external-dns-azurewebsiets\r\n  valuesInline:\r\n    provider: azure\r\n    policy: sync\r\n    domainFilters:\r\n      - azurewebsiets.net # todo: merge external-dns namespaces into single one to manage all\r\n      - \"*.azurewebsiets.net\"\r\n    azure:\r\n      resourceGroup: net.azurewebsiets\r\n      tenantId: 2e831e5f-9c6e-41a7-b295-50499684ba63\r\n      subscriptionId: 134ae9a1-7bc2-41f7-a2c7-a1e55f54ed04\r\n      useManagedIdentityExtension: true\r\n      userAssignedIdentityID: 860af6b9-4135-4814-bf81-16f3e8ebcbdd\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\workload-identity-webhook\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\n\r\nhelmCharts:\r\n  - name: workload-identity-webhook\r\n    repo: https://azure.github.io/azure-workload-identity/charts\r\n    version: 0.14.0\r\n    valuesInline:\r\n      azureTenantID: 2e831e5f-9c6e-41a7-b295-50499684ba63\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\nhelm upgrade \"teamdman\" . `\r\n    --install `\r\n    --namespace \"teamdman\" `\r\n    --create-namespace \r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cert-manager\\verify.ps1\r\n\r\n````powershell\r\n# https://cert-manager.io/docs/installation/verify/\r\ncmctl check api --wait=2m\r\nkubectl get pods --namespace cert-manager;\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\template\\aks.tf\r\n\r\n````terraform\r\n# This can take several minutes.\r\n# A bug has been fixed that made it take hours.\r\nresource \"azurerm_kubernetes_cluster\" \"main\" {\r\n  resource_group_name       = azurerm_resource_group.main.name\r\n  location                  = \"canadacentral\"\r\n  name                      = \"my-aks-cluster-1\"\r\n  sku_tier                  = \"Free\"\r\n  dns_prefix                = \"sharedcluster\"\r\n  \r\n  # az aks get-versions -l canada-central --query \"orchestrators[*].orchestratorVersion\"\r\n  kubernetes_version        = \"1.28.0\"\r\n  \r\n  oidc_issuer_enabled       = true\r\n  workload_identity_enabled = true\r\n  default_node_pool {\r\n    name       = \"default\"\r\n    node_count = 1\r\n    vm_size    = \"standard_b2s\"\r\n  }\r\n\r\n\r\n  identity {\r\n    type = \"SystemAssigned\"\r\n  }\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\templates\\service.yaml\r\n\r\n````yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: front\r\nspec:\r\n  ports:\r\n    - port: 80\r\n  selector: \r\n    app: nginx\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\templates\\ingress.yaml\r\n\r\n````yaml\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: front\r\n  annotations:\r\n    cert-manager.io/cluster-issuer: letsencrypt-prod\r\n    # cert-manager.io/issuer: letsencrypt-prod\r\nspec:\r\n  ingressClassName: nginx\r\n  # This section is only required if TLS is to be enabled for the Ingress\r\n  tls:\r\n    - hosts:\r\n        - teamdman.ca\r\n      secretName: teamdman-tls\r\n  rules:\r\n    - host: teamdman.ca\r\n      http:\r\n        paths:\r\n          - path: /\r\n            pathType: Prefix\r\n            backend:\r\n              service:\r\n                name: front\r\n                port:\r\n                  number: 80\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\templates\\secret.yaml\r\n\r\n````yaml\r\napiVersion: external-secrets.io/v1beta1\r\nkind: ExternalSecret\r\nmetadata:\r\n  name: storage-account-connection\r\nspec:\r\n  secretStoreRef:\r\n    kind: ClusterSecretStore\r\n    name: ca-teamdman\r\n  target:\r\n    name: storage-account-connection\r\n    template:\r\n      engineVersion: v2\r\n      data:\r\n        azurestorageaccountname: \"{{ `{{ .storageaccountname }}` }}\"\r\n        azurestorageaccountkey: \"{{ `{{ .storageaccountkey }}` }}\"\r\n  data:\r\n  - secretKey: storageaccountname\r\n    remoteRef:\r\n      key: storage-account-name\r\n  - secretKey: storageaccountkey\r\n    remoteRef:\r\n      key: storage-account-key\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\external-secrets\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl create ns external-secrets\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ingress-nginx\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\nkubectl create ns ingress-nginx\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\dns.tf\r\n\r\n````terraform\r\nresource \"azurerm_dns_zone\" \"main\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  name                = \"teamdman.ca\"\r\n}\r\n\r\n# resource \"azurerm_dns_a_record\" \"root\" {\r\n#   resource_group_name = azurerm_resource_group.main.name\r\n#   zone_name           = azurerm_dns_zone.main.name\r\n#   name                = \"@\"\r\n#   ttl                 = 30\r\n#   target_resource_id  = azurerm_cdn_endpoint.root.id\r\n# }\r\n\r\n## this doesn't work because of https, need a gateway or cdn endpoint\r\n# locals {\r\n#   host_name = regex(\"^https://(.*)/$\", azurerm_storage_account.main.primary_web_endpoint)[0]\r\n# }\r\n# resource \"azurerm_dns_cname_record\" \"root\" {\r\n#   resource_group_name = azurerm_resource_group.main.name\r\n#   zone_name           = azurerm_dns_zone.main.name\r\n#   name                = \"@\"\r\n#   ttl                 = 30\r\n#   # record              = azurerm_storage_account.main.primary_web_endpoint\r\n#   record              = local.host_name\r\n# }\r\n\r\nresource \"azurerm_dns_cname_record\" \"awverify\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  zone_name           = azurerm_dns_zone.main.name\r\n  name                = \"awverify\"\r\n  ttl                 = 30\r\n  record              = \"awverify.teamdman.ca.azurewebsites.net\"\r\n}\r\nresource \"azurerm_dns_cname_record\" \"awverifywww\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  zone_name           = azurerm_dns_zone.main.name\r\n  name                = \"awverify.www\"\r\n  ttl                 = 30\r\n  record              = \"awverify.teamdman.ca.azurewebsites.net\"\r\n}\r\n\r\n# resource \"azurerm_dns_cname_record\" \"cdnverify\" {\r\n#   resource_group_name = azurerm_resource_group.main.name\r\n#   zone_name           = azurerm_dns_zone.main.name\r\n#   name                = \"cdnverify\"\r\n#   ttl                 = 3600\r\n#   record              = \"cdnverify.ca-teamdman.azureedge.net\"\r\n# }\r\n\r\nresource \"azurerm_dns_cname_record\" \"wwww\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  zone_name           = azurerm_dns_zone.main.name\r\n  name                = \"wwww\"\r\n  ttl                 = 30\r\n  record              = \"teamdman.ca.azurewebsites.net\"\r\n}\r\n\r\nresource \"azurerm_role_assignment\" \"dns\" {\r\n  principal_id         = azurerm_kubernetes_cluster.main.kubelet_identity[0].object_id\r\n  scope                = azurerm_dns_zone.main.id\r\n  role_definition_name = \"DNS Zone Contributor\"\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\gifts\\identities.tf\r\n\r\n````terraform\r\nresource \"azurerm_user_assigned_identity\" \"gifts\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  name                = \"gifts\"\r\n  location            = \"canadacentral\"\r\n}\r\n\r\nresource \"azurerm_federated_identity_credential\" \"gifts\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  parent_id           = azurerm_user_assigned_identity.gifts.id\r\n  name                = \"gifts\"\r\n  issuer              = azurerm_kubernetes_cluster.main.oidc_issuer_url\r\n  subject             = \"system:serviceaccount:teamdman-gifts:gifts-sa\"\r\n  audience            = [\"api://AzureADTokenExchange\"]\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-blob-csi-driver.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: blob-csi-driver\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/blob-csi-driver\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: kube-system\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\uninstall.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\n# something keeps adding the finalizers back and I don't know why\r\n# for now we can just hard remove them before nuking the resources\r\nkubectl patch app argocd -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\nkubectl patch app app-of-apps -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\nkubectl patch app blob-csi-driver -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\nkubectl patch app cert-manager -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\nkubectl patch app external-secrets -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\nkubectl patch app ingress-nginx -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\nkubectl patch appproject core -n argocd -p '{\"metadata\":{\"finalizers\":null}}' --type=merge\r\n\r\nkubectl delete -k .\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\argocd\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\nnamespace: argocd\r\n\r\nresources:\r\n- base/certificate.yaml\r\n- base/ingress.yaml\r\n- https://raw.githubusercontent.com/argoproj/argo-cd/master/manifests/install.yaml\r\n\r\npatchesStrategicMerge:\r\n- overlays/argocd-cm.yaml\r\n# patchesStrategicMerge:\r\n# - overlays/production/argo-cd-cm.yaml\r\n# - overlays/production/argocd-server-service.yaml\r\n# - overlays/production/argocd-repo-server-deploy.yaml\r\n\r\n\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\deployment-frontend.yaml\r\n\r\n````yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: gifts-frontend\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels: \r\n      app: gifts-frontend\r\n  template:\r\n    metadata:\r\n      labels: \r\n        app: gifts-frontend\r\n    spec:\r\n      nodeSelector:\r\n        kubernetes.io/os: linux\r\n      # serviceAccountName: gifts-sa # not needed for the frontend\r\n      containers:\r\n        - name: gifts-frontend\r\n          image: teamdman.azurecr.io/gifts-frontend:latest\r\n          resources:\r\n            requests:\r\n              cpu: 100m\r\n              memory: 128Mi\r\n            limits:\r\n              cpu: 250m\r\n              memory: 256Mi\r\n          ports:\r\n            - containerPort: 3000\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\src\\modules\\cluster\\security_groups.tf\r\n\r\n````terraform\r\nvariable \"azure_ad_admins_group_name\" {\r\n  type = string\r\n}\r\nvariable \"azure_ad_admins_group_member_object_ids\" {\r\n    type = set(string)\r\n}\r\nresource \"azuread_group\" \"admins\" {\r\n    display_name = var.azure_ad_admins_group_name\r\n    members = var.azure_ad_admins_group_member_object_ids\r\n    security_enabled = true\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ingress-nginx\\uninstall.ps1\r\n\r\n````powershell\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\nkubectl delete ns ingress-nginx\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-external-secrets.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: external-secrets\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/external-secrets\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: external-secrets\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\blob-csi-driver\\uninstall.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-secretstores\\base\\cluster-secret-store.yaml\r\n\r\n````yaml\r\napiVersion: external-secrets.io/v1beta1\r\nkind: ClusterSecretStore\r\nmetadata:\r\n  name: ca-teamdman\r\nspec:\r\n  provider:\r\n    azurekv:\r\n      authType: ManagedIdentity\r\n      identityId: \"860af6b9-4135-4814-bf81-16f3e8ebcbdd\"\r\n      tenantId: \"2e831e5f-9c6e-41a7-b295-50499684ba63\"\r\n      vaultUrl: \"https://ca-teamdman.vault.azure.net/\"\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\pod.yaml\r\n\r\n````yaml\r\n# https://github.com/Azure/azure-workload-identity/blob/main/examples/msal-net/akvdotnet/Program.cs\r\napiVersion: v1\r\nkind: Pod\r\nmetadata:\r\n  name: quick-start\r\nspec:\r\n  serviceAccountName: gifts-sa\r\n  containers:\r\n    - image: ghcr.io/azure/azure-workload-identity/msal-go\r\n      name: oidc\r\n      env:\r\n      - name: KEYVAULT_URL\r\n        value: https://ca-teamdman-gifts.vault.azure.net/\r\n      - name: SECRET_NAME\r\n        value: StorageTableName\r\n      # - name: AZURE_CLIENT_ID\r\n      #   value: edbcd52a-f602-43be-951c-16d6cf7c7146\r\n  nodeSelector:\r\n    kubernetes.io/os: linux\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\email-forwarder\\deploy.yaml\r\n\r\n````yaml\r\napiVersion: apps/v1\r\nkind: Deployment\r\nmetadata:\r\n  name: email-forwarder\r\n  labels:\r\n    app: email-forwarder\r\nspec:\r\n  replicas: 1\r\n  selector:\r\n    matchLabels:\r\n        app: email-forwarder\r\n  template:\r\n    metadata:\r\n      labels:\r\n        app: email-forwarder\r\n    spec:\r\n      containers:\r\n      - name: email-forwarder\r\n        image: zixia/simple-mail-forwarder:1.4\r\n        ports:\r\n        - containerPort: 25\r\n---\r\napiVersion: networking.k8s.io/v1\r\nkind: Ingress\r\nmetadata:\r\n  name: mail-forwarder\r\nspec:\r\n  ingressClassName: nginx\r\n  rules:\r\n  - http:\r\n      paths:\r\n      - path: /\r\n        pathType: Prefix\r\n        backend:\r\n          service:\r\n            name: test\r\n            port:\r\n              number: 25\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\aks\\get-env.ps1\r\n\r\n````powershell\r\n# $pat = op read \"op://Private/Github terraform access token/password\"\r\n$pat = gh auth token\r\nSet-Content \".env\" \"github_token=$pat\"\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\teamdman\\app-teamdman-www.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: teamdman-www\r\n  namespace: argocd\r\nspec:\r\n  project: teamdman\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/ca-teamdman-www\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: teamdman\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-issuers\\base\\clusterissuer-letsencrypt-staging.yaml\r\n\r\n````yaml\r\napiVersion: cert-manager.io/v1\r\nkind: ClusterIssuer\r\nmetadata:\r\n  name: letsencrypt-staging\r\nspec:\r\n  acme:\r\n    email: TeamDman9201@gmail.com\r\n    server: https://acme-staging-v02.api.letsencrypt.org/directory\r\n    privateKeySecretRef:\r\n      # Secret resource that will be used to store the account's private key.\r\n      name: cert-manager-issuer-account-key-staging\r\n    solvers:\r\n    - dns01:\r\n        azureDNS:\r\n          subscriptionID: 134ae9a1-7bc2-41f7-a2c7-a1e55f54ed04\r\n          resourceGroupName: ca.teamdman\r\n          hostedZoneName: teamdman.ca\r\n          environment: AzurePublicCloud\r\n          managedIdentity:\r\n            clientID: 860af6b9-4135-4814-bf81-16f3e8ebcbdd\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\src\\module_impls.tf\r\n\r\n````terraform\r\ndata \"azurerm_client_config\" \"main\" {}\r\n\r\nmodule \"benthic_cluster\" {\r\n  source                                  = \"./modules/cluster\"\r\n  resource_group_name                     = \"CACN-Cluster-Benthic-PROD-RG\"\r\n  resource_group_location                 = \"canadacentral\"\r\n  tag_environment                         = \"Production\"\r\n  cluster_name                            = \"Benthic-PROD-AKS\"\r\n  cluster_dns_prefix                      = \"benthic-prod\"\r\n  azure_ad_admins_group_name              = \"Benthic-PROD-Admins\"\r\n  azure_ad_admins_group_member_object_ids = [data.azurerm_client_config.main.object_id]\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\service-backend.yaml\r\n\r\n````yaml\r\napiVersion: v1\r\nkind: Service\r\nmetadata:\r\n  name: gifts-backend\r\nspec:\r\n  ports:\r\n    - port: 80\r\n  selector: \r\n    app: gifts-backend\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\gifts\\keyvault.tf\r\n\r\n````terraform\r\nresource \"azurerm_key_vault\" \"main\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  location            = \"canadaeast\"\r\n  tenant_id           = data.azurerm_client_config.current.tenant_id\r\n  sku_name            = \"standard\"\r\n  name                = \"ca-teamdman\"\r\n}\r\n\r\nresource \"azurerm_key_vault_secret\" \"main\" {\r\n  for_each = {\r\n    \"storage-account-name\" = azurerm_storage_account.main.name\r\n    \"storage-account-key\"  = azurerm_storage_account.main.primary_access_key\r\n  }\r\n  key_vault_id = azurerm_key_vault.main.id\r\n  name         = each.key\r\n  value        = each.value\r\n  depends_on = [\r\n    azurerm_key_vault_access_policy.me\r\n  ]\r\n}\r\n\r\nresource \"azurerm_key_vault_access_policy\" \"external-secrets-operator\" {\r\n  key_vault_id       = azurerm_key_vault.main.id\r\n  secret_permissions = [\"Get\", \"List\"]\r\n  tenant_id          = azurerm_key_vault.main.tenant_id\r\n  object_id          = azurerm_kubernetes_cluster.main.kubelet_identity[0].object_id\r\n}\r\n\r\n###\r\n\r\nresource \"azurerm_key_vault\" \"gifts\" {\r\n  resource_group_name = azurerm_resource_group.main.name\r\n  location            = \"canadaeast\"\r\n  tenant_id           = data.azurerm_client_config.current.tenant_id\r\n  sku_name            = \"standard\"\r\n  name                = \"ca-teamdman-gifts\"\r\n}\r\n\r\nresource \"random_password\" \"jwt_key\" {\r\n  length  = 256\r\n  special = true\r\n}\r\nresource \"azurerm_key_vault_secret\" \"gifts\" {\r\n  for_each = {\r\n    \"StorageConnectionString\" = azurerm_storage_account.main.primary_connection_string\r\n    \"JwtKey\"                  = random_password.jwt_key.result\r\n    \"JwtIssuer\"               = \"https://gifts.teamdman.ca/\"\r\n    \"JwtAudience\"             = \"https://gifts.teamdman.ca/\"\r\n    # \"storage-account-name\" = azurerm_storage_account.main.name\r\n    # \"storage-account-key\"  = azurerm_storage_account.main.primary_access_key\r\n    # \"storage-account-table\" = azurerm_storage_table.gifts.name\r\n  }\r\n  key_vault_id = azurerm_key_vault.gifts.id\r\n  name         = each.key\r\n  value        = each.value\r\n  depends_on = [\r\n    azurerm_key_vault_access_policy.me\r\n  ]\r\n}\r\n\r\n\r\n# resource \"azurerm_key_vault_access_policy\" \"gifts\" {\r\n#   key_vault_id       = azurerm_key_vault.gifts.id\r\n#   tenant_id          = azurerm_key_vault.gifts.tenant_id\r\n#   object_id = azurerm_user_assigned_identity.gifts.principal_id\r\n#   secret_permissions = [\"Get\", \"List\"]\r\n# }\r\n\r\n###\r\n\r\n# this does not scale to team projects\r\nresource \"azurerm_key_vault_access_policy\" \"me\" {\r\n  for_each = {\r\n    \"main\"  = azurerm_key_vault.main.id\r\n    \"gifts\" = azurerm_key_vault.gifts.id\r\n  }\r\n  key_vault_id = each.value\r\n  secret_permissions = [\r\n    \"Backup\", \"Delete\", \"Get\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Set\"\r\n  ]\r\n  certificate_permissions = [\r\n    \"Backup\", \"Create\", \"Delete\", \"DeleteIssuers\", \"Get\", \"GetIssuers\", \"Import\", \"List\", \"ListIssuers\", \"ManageContacts\", \"ManageIssuers\", \"Purge\", \"Recover\", \"Restore\", \"SetIssuers\", \"Update\"\r\n  ]\r\n  key_permissions = [\r\n    \"Backup\", \"Create\", \"Decrypt\", \"Delete\", \"Encrypt\", \"Get\", \"Import\", \"List\", \"Purge\", \"Recover\", \"Restore\", \"Sign\", \"UnwrapKey\", \"Update\", \"Verify\", \"WrapKey\"\r\n  ]\r\n  tenant_id = azurerm_key_vault.main.tenant_id\r\n  object_id = data.azurerm_client_config.current.object_id\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\core\\app-argocd.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: Application\r\nmetadata:\r\n  name: argocd\r\n  namespace: argocd\r\nspec:\r\n  project: core\r\n\r\n  # Source of the application manifests\r\n  source:\r\n    repoURL: https://github.com/teamdman-ca/infrastructure-as-code.git\r\n    targetRevision: HEAD\r\n    path: kubernetes/argocd\r\n\r\n  # Destination cluster and namespace to deploy the application\r\n  destination:\r\n    server: https://kubernetes.default.svc\r\n    namespace: argocd\r\n\r\n  # Sync policy\r\n  syncPolicy:\r\n    # automated: # automated sync by default retries failed attempts 5 times with following delays between attempts ( 5s, 10s, 20s, 40s, 80s ); retry controlled using `retry` field.\r\n    #   prune: true # Specifies if resources should be pruned during auto-syncing ( false by default ).\r\n    #   selfHeal: true # Specifies if partial app sync should be executed when resources are changed only in target Kubernetes cluster and no git change detected ( false by default ).\r\n    #   allowEmpty: true # Allows deleting all application resources during automatic syncing ( false by default ).\r\n    syncOptions:     # Sync options which modifies sync behavior\r\n    - Validate=false # disables resource validation (equivalent to 'kubectl apply --validate=false') ( true by default ).\r\n    - CreateNamespace=true # Namespace Auto-Creation ensures that namespace specified as the application destination exists in the destination cluster.\r\n    - PrunePropagationPolicy=foreground # Supported policies are background, foreground and orphan.\r\n    - PruneLast=true # Allow the ability for resource pruning to happen as a final, implicit wave of a sync operation\r\n    # The retry feature is available since v1.7\r\n    retry:\r\n      limit: 0 # number of failed sync attempt retries; unlimited number of attempts if less than 0\r\n      backoff:\r\n        duration: 5s # the amount to back off. Default unit is seconds, but could also be a duration (e.g. \"2m\", \"1h\")\r\n        factor: 2 # a factor to multiply the base duration after each failed retry\r\n        maxDuration: 3m # the maximum amount of time allowed for the backoff strategy\r\n\r\n  # # Will ignore differences between live and desired states during the diff. Note that these configurations are not\r\n  # # used during the sync process.\r\n  # ignoreDifferences:\r\n  # # for the specified json pointers\r\n  # - group: apps\r\n  #   kind: Deployment\r\n  #   jsonPointers:\r\n  #   - /spec/replicas\r\n  # # for the specified managedFields managers\r\n  # - group: \"*\"\r\n  #   kind: \"*\"\r\n  #   managedFieldsManagers:\r\n  #   - kube-controller-manager\r\n\r\n  # RevisionHistoryLimit limits the number of items kept in the application's revision history, which is used for\r\n  # informational purposes as well as for rollbacks to previous versions. This should only be changed in exceptional\r\n  # circumstances. Setting to zero will store no history. This will reduce storage used. Increasing will increase the\r\n  # space used to store the history, so we do not recommend increasing it.\r\n  revisionHistoryLimit: 10\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\blob-csi-driver\\kustomization.yaml\r\n\r\n````yaml\r\napiVersion: kustomize.config.k8s.io/v1beta1\r\nkind: Kustomization\r\n\r\nhelmCharts:\r\n- name: blob-csi-driver\r\n  repo: https://raw.githubusercontent.com/kubernetes-sigs/blob-csi-driver/master/charts\r\n  version: v1.18.0\r\n  namespace: kube-system\r\n  releaseName: blob-csi-driver\r\n  valuesInline:\r\n    controller:\r\n      replicas: 1\r\n    node.enableBlobfuseProxy: true\r\n    cloud: AzureStackCloud\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-raddest-gifts\\base\\service-account.yaml\r\n\r\n````yaml\r\napiVersion: v1\r\nkind: ServiceAccount\r\nmetadata:\r\n  name: gifts-sa\r\n  annotations:\r\n    azure.workload.identity/client-id: edbcd52a-f602-43be-951c-16d6cf7c7146\r\n  labels:\r\n    azure.workload.identity/use: \"true\"\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\src\\modules\\cluster\\resource_group.tf\r\n\r\n````terraform\r\nvariable \"resource_group_name\" {\r\n  type = string\r\n}\r\nvariable \"resource_group_location\" {\r\n  type = string\r\n}\r\nvariable \"tag_environment\" {\r\n  type = string\r\n}\r\nresource \"azurerm_resource_group\" \"main\" {\r\n  name     = var.resource_group_name\r\n  location = var.resource_group_location\r\n  tags = {\r\n    environment = var.tag_environment\r\n  }\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\ca-teamdman-www\\get-cert-status.ps1\r\n\r\n````powershell\r\nWrite-Host \"Certificates\" -ForegroundColor Cyan\r\nkubectl get certificate -n azurewebsiets -o wide\r\n\r\nWrite-Host \"`nCertificate Requests\" -ForegroundColor Cyan\r\nkubectl get certificaterequest -n azurewebsiets -o wide\r\n\r\nWrite-Host \"`nOrders\" -ForegroundColor Cyan\r\nkubectl get order -n azurewebsiets -o wide\r\n\r\nWrite-Host \"`nChallenges\" -ForegroundColor Cyan\r\nkubectl get challenge -n azurewebsiets -o wide\r\n\r\n$decision = $Host.UI.PromptForChoice(\"Describe resources\",\"Describe a resource?\", @(\"&Cert\",\"&Request\", \"&Order\", \"Cha&llenge\"), 0)\r\nif ($decision -eq 0)\r\n{\r\n    $name = kubectl get certificate -n azurewebsiets -o name\r\n    kubectl describe -n azurewebsiets $name\r\n}\r\nelseif ($decision -eq 1)\r\n{\r\n    $name = kubectl get certificaterequest -n azurewebsiets -o name\r\n    kubectl describe -n azurewebsiets $name\r\n}\r\nelseif ($decision -eq 2)\r\n{\r\n    $name = kubectl get order -n azurewebsiets -o name\r\n    kubectl describe -n azurewebsiets $name\r\n}\r\nelseif ($decision -eq 3)\r\n{\r\n    $name = kubectl get challenge -n azurewebsiets -o name\r\n    kubectl describe -n azurewebsiets $name\r\n}\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cert-manager\\list.ps1\r\n\r\n````powershell\r\nkubectl get Issuers,ClusterIssuers,Certificates,CertificateRequests,Orders,Challenges --all-namespaces\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\cluster-issuers\\uninstall.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl kustomize --enable-helm | kubectl delete -f -\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\namecheap\\main.tf\r\n\r\n````terraform\r\nprovider \"namecheap\" {\r\n  user_name   = \"TeamDman\"\r\n  api_user    = \"user\"\r\n  api_key     = \"key\"\r\n  client_ip   = \"123.123.123.123\"\r\n  use_sandbox = true\r\n}\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\app-of-apps\\base\\azurewebsiets\\project-azurewebsiets.yaml\r\n\r\n````yaml\r\napiVersion: argoproj.io/v1alpha1\r\nkind: AppProject\r\nmetadata:\r\n  name: azurewebsiets\r\nspec:\r\n  description: Core resources to the cluster\r\n\r\n  # Allow manifests to deploy from any Git repos\r\n  sourceRepos:\r\n  - '*'\r\n\r\n  destinations:\r\n  - namespace: azurewebsiets\r\n    server: https://kubernetes.default.svc\r\n  - namespace: external-dns-azurewebsiets\r\n    server: https://kubernetes.default.svc\r\n\r\n  # Deny all cluster-scoped resources from being created, except for Namespace\r\n  clusterResourceWhitelist:\r\n  - group: ''\r\n    kind: Namespace\r\n\r\n  # Allow all namespaced-scoped resources to be created, except for ResourceQuota, LimitRange, NetworkPolicy\r\n  namespaceResourceBlacklist:\r\n  - group: ''\r\n    kind: ResourceQuota\r\n  - group: ''\r\n    kind: LimitRange\r\n  - group: ''\r\n    kind: NetworkPolicy\r\n\r\n  # # Deny all namespaced-scoped resources from being created, except for Deployment and StatefulSet\r\n  # namespaceResourceWhitelist:\r\n  # - group: 'apps'\r\n  #   kind: Deployment\r\n  # - group: 'apps'\r\n  #   kind: StatefulSet\r\n\r\n  # Enables namespace orphaned resource monitoring.\r\n  orphanedResources:\r\n    warn: false\r\n\r\n````\r\n\r\n\r\n\r\n## D:\\Repos\\Azure\\ca.teamdman.iac\\old\\old\\kubernetes\\blob-csi-driver\\install.ps1\r\n\r\n````powershell\r\n#!/usr/bin/pwsh\r\n\r\nkubectl kustomize --enable-helm | kubectl apply -f -\r\n# helm install blob-csi-driver blob-csi-driver/blob-csi-driver --set node.enableBlobfuseProxy=true --namespace kube-system --set cloud=AzureStackCloud\r\n````\r\n\r\n\r\n\r\nThis is the current state my of azurewebsiets repository.\r\nSince the \"old\" version, I have deleted everything in my test tenant to start over.\r\nIn the init folder, we create the storage account to hold state files.\r\nIn src, we use that storage account as the backend bucket for the state file, and we deploy the cluster.\r\nThe old version was primed to be used with ArgoCD, but instead we are using Terraform to manage all aspects of the cluster.\r\nThe current goal is to get nginx running and accessible from a public IP.\r\nTo do this, we have\r\n- azurerm_kubernetes_cluster which uses an azuread_security_group to hold the list of admins, which is populated with azurerm_client_config to get the identity of the user running the script.\r\n- azurerm_dnz_zone to hold the DNS entries for our site, which is an intentional typo of \"azurewebsites.net\".\r\n- A commented out role assignment for the cluter to do DNS\r\n\r\nInstead of using techniques like mounting a storage account using blob-csi-driver, and instead of creating dns entries using external-dns, I am interested in a scenario where it is all controlled with terraform.\r\nThat is, instead of writing yaml to link everything together, we want to use the strong typing that terraform has.\r\n\r\nWe will need a namespace for the nginx hello world pod to run.\r\nThis namespace needs to be created after the creation of the cluster.\r\nPropose a new module that will accept the necessary parameters as variables, such as cluster name and connection strings, and will create the namespace.\r\nWe will need to modify the cluster module to add outputs for relevant information.\r\n\r\nPropose an overall plan, then \r\ngive step by step instructions for what modifications need to be made where to successfully perform the state transition to the end of the plan.\r\n\r\n",
        "files": []
    },
    "response": {
        "candidates": [
            {
                "content": {
                    "parts": [
                        {
                            "text": "## Plan for Creating a Kubernetes Namespace with Terraform\n\nHere's a proposed plan to create a Kubernetes namespace using Terraform, along with the required modifications to your existing modules:\n\n**1. Modify Cluster Module:**\n\n*   **Add Outputs:** Include outputs for the following information from the `azurerm_kubernetes_cluster` resource:\n    *   `kube_config`: This provides the necessary configuration to interact with the cluster.\n    *   `cluster_id`: The unique ID of the cluster.\n\n**2. Create Namespace Module:**\n\n*   **Module Inputs:** Define variables to accept:\n    *   `cluster_id`: (Required) The ID of the AKS cluster.\n    *   `kube_config`: (Required) The Kubernetes configuration data.\n    *   `namespace_name`: (Required) The name of the namespace to be created.\n\n*   **Resource Creation:** Use the `kubernetes_namespace` resource within the module to create the namespace:\n\n```terraform\nresource \"kubernetes_namespace\" \"this\" {\n  metadata {\n    name = var.namespace_name\n  }\n}\n\n# Depend on the provider configuration\ndepends_on = [\n    kubernetes_provider.main\n]\n```\n\n*   **Provider Configuration:** Configure the `kubernetes` provider using the received `kube_config` data:\n\n```terraform\nprovider \"kubernetes\" {\n  host                   = var.kube_config.host\n  cluster_ca_certificate = base64decode(var.kube_config.cluster_ca_certificate)\n  token                  = var.kube_config.token\n  load_config_file       = false\n}\n```\n\n**3. Implement the Namespace Module:**\n\n*   In your `module_impls.tf` or a similar file, call the newly created namespace module and pass the required variables obtained from the cluster module outputs. \n\n```terraform\nmodule \"nginx_namespace\" {\n  source       = \"./modules/namespace\"\n  cluster_id   = module.benthic_cluster.cluster_id\n  kube_config  = module.benthic_cluster.kube_config\n  namespace_name = \"nginx\"\n}\n```\n\n## Step-by-Step Instructions:\n\n1.  **Modify `outputs.tf` in the Cluster Module:**\n    *   Add the following outputs to the `outputs.tf` file within your cluster module:\n\n    ```terraform\n    output \"kube_config\" {\n      value = azurerm_kubernetes_cluster.main.kube_config_raw\n    }\n\n    output \"cluster_id\" {\n      value = azurerm_kubernetes_cluster.main.id\n    }\n    ```\n\n2.  **Create Namespace Module:**\n    *   Create a new directory named `namespace` within your modules directory.\n    *   Inside the `namespace` directory, create the following files:\n        *   `variables.tf`: Define the input variables as described in the plan above. \n        *   `main.tf`: Add the `kubernetes_namespace` resource and the `kubernetes` provider configuration.\n\n3.  **Implement the Namespace Module:**\n    *   In your `module_impls.tf` (or a similar file where you call your modules), call the `nginx_namespace` module and pass the required variables from the cluster module outputs as shown in the example above.\n\n4.  **Run Terraform:**\n    *   Execute `terraform init` to initialize the new module.\n    *   Run `terraform plan` to review the planned changes.\n    *   Finally, run `terraform apply` to create the namespace. \n\n## Additional Notes:\n\n*   You can expand the namespace module to accommodate additional configurations or resources needed within the namespace, such as deployments or services. \n*   Remember to adjust the module names and variable names to match your specific project structure and naming conventions. \n\nBy following these steps, you'll be able to create Kubernetes namespaces with Terraform and manage your infrastructure more effectively. \n"
                        }
                    ],
                    "role": "model"
                },
                "finishReason": "STOP",
                "index": 0,
                "safetyRatings": [
                    {
                        "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        "probability": "NEGLIGIBLE"
                    },
                    {
                        "category": "HARM_CATEGORY_HATE_SPEECH",
                        "probability": "NEGLIGIBLE"
                    },
                    {
                        "category": "HARM_CATEGORY_HARASSMENT",
                        "probability": "NEGLIGIBLE"
                    },
                    {
                        "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                        "probability": "NEGLIGIBLE"
                    }
                ]
            }
        ]
    }
}