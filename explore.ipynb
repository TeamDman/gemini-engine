{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    assert api_key\n",
    "except NameError:\n",
    "    api_key = !op read \"op://Private/Google AI Studio API key/credential\"\n",
    "    assert api_key\n",
    "    api_key = api_key[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "url = f\"https://generativelanguage.googleapis.com/v1beta/models?key={api_key}\"\n",
    "models = requests.get(url).json()[\"models\"]\n",
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[f\"{x['name']} {prop} {x[prop]}\" for x in models for prop in [\"inputTokenLimit\",\"outputTokenLimit\"] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q google-api-python-client google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import googleapiclient\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.http import MediaFileUpload\n",
    "import mimetypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENAI_DISCOVERY_URL = f\"https://generativelanguage.googleapis.com/$discovery/rest?version=v1beta&key={api_key}\"\n",
    "# Initialize Google API Client\n",
    "discovery_docs = requests.get(GENAI_DISCOVERY_URL)\n",
    "genai_service = googleapiclient.discovery.build_from_document(\n",
    "    discovery_docs.content, developerKey=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mimetypes.guess_type(\"abc.webp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare file to upload to GenAI File API\n",
    "file_path = \"frieren.png\"\n",
    "\n",
    "media = MediaFileUpload(file_path, mimetype=mimetypes.guess_type(file_path)[0])\n",
    "body = {\"file\": {\"displayName\": \"Frieren meme\"}}\n",
    "\n",
    "# Upload file\n",
    "create_file_request = genai_service.media().upload(media_body=media, body=body)\n",
    "create_file_response = create_file_request.execute()\n",
    "file_uri = create_file_response[\"file\"][\"uri\"]\n",
    "file_mimetype = create_file_response[\"file\"][\"mimeType\"]\n",
    "print(\"Uploaded file: \" + file_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Gemini 1.5 API LLM call\n",
    "prompt = \"Describe the image with a creative description\"\n",
    "model = \"models/gemini-1.5-pro-latest\"\n",
    "contents = {\"contents\": [{ \n",
    "  \"parts\":[\n",
    "    {\"text\": prompt},\n",
    "    {\"file_data\": {\"file_uri\": file_uri, \"mime_type\": file_mimetype}}]\n",
    "}]}\n",
    "genai_request = genai_service.models().generateContent(model=model, body=contents)\n",
    "resp = genai_request.execute()\n",
    "print(len(resp[\"candidates\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display,Image\n",
    "from pprint import pprint\n",
    "display(Image(file_path, width=400))\n",
    "pprint(resp[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare file to upload to GenAI File API\n",
    "file_path = \"beans.txt\"\n",
    "\n",
    "# media = MediaFileUpload(file_path, mimetype=\"application/json\")\n",
    "media = MediaFileUpload(file_path, mimetype=mimetypes.guess_type(file_path)[0])\n",
    "body = {\"file\": {\"displayName\": \"A text file\"}}\n",
    "\n",
    "# Upload file\n",
    "create_file_request = genai_service.media().upload(media_body=media, body=body)\n",
    "create_file_response = create_file_request.execute()\n",
    "file_uri = create_file_response[\"file\"][\"uri\"]\n",
    "file_mimetype = create_file_response[\"file\"][\"mimeType\"]\n",
    "print(\"Uploaded file: \" + file_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make Gemini 1.5 API LLM call\n",
    "prompt = \"Describe the contents of the text file\"\n",
    "model = \"models/gemini-1.5-pro-latest\"\n",
    "contents = {\"contents\": [{ \n",
    "  \"parts\":[\n",
    "    {\"text\": prompt},\n",
    "    {\"file_data\": {\"file_uri\": file_uri, \"mime_type\": file_mimetype}}]\n",
    "}]}\n",
    "genai_request = genai_service.models().generateContent(model=model, body=contents)\n",
    "resp = genai_request.execute()\n",
    "print(len(resp[\"candidates\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare file to upload to GenAI File API\n",
    "file_path = \"explore.ipynb\"\n",
    "\n",
    "# media = MediaFileUpload(file_path, mimetype=\"application/json\")\n",
    "media = MediaFileUpload(file_path, mimetype=\"text/plain\")\n",
    "body = {\"file\": {\"displayName\": \"This jupyter notebook\"}}\n",
    "\n",
    "# Upload file\n",
    "create_file_request = genai_service.media().upload(media_body=media, body=body)\n",
    "create_file_response = create_file_request.execute()\n",
    "file_uri = create_file_response[\"file\"][\"uri\"]\n",
    "file_mimetype = create_file_response[\"file\"][\"mimeType\"]\n",
    "print(\"Uploaded file: \" + file_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_mimetype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "this_notebook = open(\"explore.ipynb\", \"r\").read()\n",
    "\n",
    "# Make Gemini 1.5 API LLM call\n",
    "prompt = \"\"\"I'm just getting started with the Gemini 1.5 API. I know that uploading files and caching the url is going to be an important part of the workflow.\n",
    "Perhaps a thing like file_path_to_remote_url_mapping = {} with a helper method to get-or-create entry will be useful.\n",
    "The current goal is to streamline the file upload process in a way that I can just give it a file path and it will cache the remote url.\n",
    "Then, I can create a prompt and evaluate it against any of the cached files.\n",
    "We can use fzf to select the file to evaluate against.\n",
    "The file uploading and inference API calls are python components, but the fzf call will be a pwsh invocation.\n",
    "So, we need to create a .py file we can invoke with pwsh to upload a file and get the url, \n",
    "then in pwsh we can use a json file to act as a simple dictionary to cache the file urls\n",
    "lets imagine the gemini.ps1 file to output something like\n",
    "\n",
    "Welcome to the Gemini 1.5 API\n",
    "Found 23 files. (read from files.json)\n",
    "Found 13 prompts. (read from prompts/*.txt)\n",
    "\n",
    "1. Upload a file\n",
    "2. Create a prompt\n",
    "3. Execute a prompt\n",
    "\n",
    "Choice: 1\n",
    "Enter the file path: explore.ipynb\n",
    "\"explore.ipynb\" already exists, overwrite? (y/n): y\n",
    "Uploaded file and saved the url to the cache.\n",
    "\n",
    "Found 24 files. (read from files.json)\n",
    "Found 13 prompts. (read from prompts/*.txt)\n",
    "\n",
    "1. Upload a file\n",
    "2. Create a prompt\n",
    "3. Execute a prompt\n",
    "\n",
    "Choice: 2\n",
    "\n",
    "Enter a name for the prompt: \"Describe the jupyter notebook\"\n",
    "(here, the pwsh file calls `hx prompts/${name}.txt` to open the file in the helix CLI editor, blocking until the user finishes)\n",
    "\n",
    "Found 24 files. (read from files.json)\n",
    "Found 14 prompts. (read from prompts/*.txt)\n",
    "\n",
    "1. Upload a file\n",
    "2. Create a prompt\n",
    "3. Execute a prompt\n",
    "\n",
    "Choice: 3\n",
    "\n",
    "(here, the pwsh file does something like\n",
    "$chosen_file_name = $cached_file_names | fzf\n",
    "$file_url = $cached_file_urls[$chosen_file_name]\n",
    "try {\n",
    "Push-Location prompts\n",
    "$chosen_prompt_name = $cached_prompt_names | fzf\n",
    "} finally {\n",
    "Pop-Location\n",
    "}\n",
    "$prompt = Get-Content prompts\\$chosen_prompt_name\n",
    "payload = [pscustomobject]@{\n",
    "  prompt = $prompt\n",
    "  file_url = $file_url\n",
    "}\n",
    "python gemini_invoke.py $($payload | ConvertTo-Json)\n",
    "\n",
    "Please, using markdown syntax with code blocks, write the code for the following files:\n",
    "- gemini.ps1\n",
    "- gemini_file_upload.py\n",
    "- gemini_invoke.py\n",
    "\n",
    "Let's think step by step.\n",
    "For each file, write an English requirements paragraph that describes the decision flows and expectations of the connected components.\n",
    "After writing the requirements for the individual files, review the interdependencies for any requirements that are not yet clear.\n",
    "\n",
    "Finally, for each file, output the code similar to the following\n",
    "\n",
    "```gemini.ps1\n",
    "%%%CODE%%%\n",
    "```\n",
    "\n",
    "and replace `%%%CODE%%%` with the code that fulfills the requirements.\n",
    "\n",
    "Finally, here is the content of this jupyter notebook\n",
    "===\n",
    "\"\"\" + this_notebook\n",
    "print(len(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(prompt) < 100_000, \"Make sure to clear cell outputs containing images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"models/gemini-1.5-pro-latest\"\n",
    "contents = {\n",
    "    \"contents\": [\n",
    "        {\n",
    "            \"parts\": [\n",
    "                {\"text\": prompt},\n",
    "                # {\"file_data\": {\"file_uri\": file_uri, \"mime_type\": file_mimetype}}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "genai_request = genai_service.models().generateContent(model=model, body=contents)\n",
    "resp = genai_request.execute()\n",
    "print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(resp[\"candidates\"]))\n",
    "pprint(resp[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"reply.md\",\"w\") as f:\n",
    "    f.write(resp[\"candidates\"][0][\"content\"][\"parts\"][0][\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sfm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
